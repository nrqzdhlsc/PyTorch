----

原文链接：https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py

译者：BING

时间：20190528

-----

在PyTorch中`autograd`包是所有神经网络的核心。我们先简单看看这个包，然后开始训练我们的第一个神经网络。

`autograd`包为**在张量上的所有操作**提供了自动求导计算。这是由运行框架定义的，也就说反向传播会由代码运行时决定，每一次的迭代都可能不同。

我们先通过一些案例来探究一下。

## 张量

`torch.Tensor`是这个包的核心类。如果这是它的属性`.require_grad`为`True`，它就会跟踪在它上面的所有操作。当计算完成后，你可以调用`.backward()`方法自动得到所有的梯度。该张量的梯度会存储到`.grad`属性中。

若想停止跟踪历史，可以调用`.detach()`从计算历史中解除关联，并阻止未来的计算跟踪。

若想阻止跟踪历史(以及使用内存)，可以将代码块放在`with torch.no_grad():`中。这在评估模型时尤其有用，因为模型有一些训练参数，需要`requires_grad=True`，但是评估时并不需要梯度。

自动梯度的实现，有一个很重要的类，`Function`。

`Tensor`和`Funtion`是相互连接的，一起构建了无环图，会对计算历史进行编码。每个张量有一个`.grad_fn`属性，指向`Function`，这个函数创建了张量（除了这些由用户创建的张量--他们的`grad_fn`是`None`）。

如果你想计算导数，可以在张量上调用`.backward()`。如果张量是标量，就不用在函数`.backward()`上指定任何参数。但是，如果张量有很多元素，就需要指定一个`gradient`参数，这是一个形状匹配的张量。

```python
import torch
```

创建张量并设置`requires_grad=True` 以跟踪它上面的运算：

```python
x = torch.ones(2, 2, requires_grad=True)
print(x)
```

输出:

```bash
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
```

执行张量计算：

```python
y = x + 2
print(y)
```

输出:

```bash
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
```

`y` 是由操作结果创建的，因此它有一个`grad_fn`属性。

```python
print(y.grad_fn)
```

输出:

```bash
<AddBackward0 object at 0x7f7799d867f0>
```

在`y`上再做一些运算：

```python
z = y * y * 3
out = z.mean()

print(z, out)
```

输出:

```bash
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)
```

`.requires_grad_( ... )`会在当前张量上直接修改张量的`requires_grad`属性。如果没有给定这个属性，默认值是`False`。

```python
a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
```

输出:

```python
False
True
<SumBackward0 object at 0x7f7799d1e898>
```

## 梯度

现在执行反向传播。因为`out`包含一个标量，`out.backward()`就等同于`out.backward(torch.tensor(1.))`。

```python
out.backward()
```

打印梯度$\frac{d(out)}{dx}$：

```python
print(x.grad)
```

Out:

```bash
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
```


应该得到一个值为4.5的矩阵。我们把`out`张量称作`o`，现在$o = \frac{1}{4}\sum_i z_i, i = 3(x_i + 2)^2$并且$z_i|_{x_i} = 27$。

因此，$\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i + 2)$，因此$\frac{\partial o}{\partial x_i} = \frac{9}{2} = 4.5$。

从数学上看，如果有一个向量函数$\overrightarrow y = f(\overrightarrow x)$，那么$\overrightarrow y$对$\overrightarrow x$的梯度是个雅各比矩阵：
$$
J = \left[
      \begin{matrix}
      \frac{\partial y_1}{\partial x_1}& ... & \frac{\partial y_1}{\partial x_1} \\
      . & . & . \\
      \frac{\partial y_m}{\partial x_1}& ... & \frac{\partial y_m}{\partial x_n} \\
      \end{matrix}
		\right]
$$


通常来说，`torch.autograd`是用于计算向量和雅各比矩阵乘积的引擎。也就是说给定一个向量，$v = (v_1, v_2, ..., v_m)^T$计算乘积$v^T \cdot J $。如果$v$恰好是标量函数$l = g(\overrightarrow y)$的梯度，即$v  = (\frac{\partial l}{\partial y1} ... \frac{\partial l}{\partial ym})^T$，根据链式法则，向量与雅各比矩阵的乘积就是$l$对$\overrightarrow x$的梯度：
$$
v^T \cdot J  = \left[
      \begin{matrix}
      \frac{\partial y_1}{\partial x_1}& ... & \frac{\partial y_m}{\partial x_1} \\
      . & . & . \\
      \frac{\partial y_1}{\partial x_n}& ... & \frac{\partial y_m}{\partial x_n} \\
      \end{matrix}
		\right] 
		
		\left[
      \begin{matrix}
      \frac{\partial l}{\partial y_1}\\
      . \\
      \frac{\partial l}{\partial y_m}
      \end{matrix}
		\right] 
		= 
		\left[
      \begin{matrix}
      \frac{\partial l}{\partial x_1}\\
      . \\
      \frac{\partial l}{\partial x_n}
      \end{matrix}
		\right] 
$$



注意向量与雅各比矩阵的乘积的特征使得很容易就能将外部的梯度填充到非标量输出的模型中。

现在我们看一个向量与雅各比矩阵乘积的例子：

```python
x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() < 1000:
    y = y * 2

print(y)
```

输出:

```bash
tensor([-349.2490,  492.2512, 1086.6757], grad_fn=<MulBackward0>)
```

这个例子里，`y`不再是标量，`torch.autograd`不能直接计算完整的雅各比矩阵，但是我们只是想要向量和雅各比矩阵的乘积，将向量作为`backward`函数的参数传入即可。

```python
v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(v)

print(x.grad)
```

输出:

```bash
tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])
```

可以停止跟踪张量上的自动梯度计算，虽然张量设置为`.requires_grad=True`，只要放在代码块`withtorch.no_grad():`中就能停止梯度计算。

```python
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)
```

输出:

```bash
True
True
False
```

**后续阅读:**

关于 `autograd` 和 `Function`文档在 https://pytorch.org/docs/autograd。

END.