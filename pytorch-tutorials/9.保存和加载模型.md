----

原文链接：https://pytorch.org/tutorials/beginner/saving_loading_models.html

译者：BING

时间：20190604

----

**作者:** [Matthew Inkawhich](https://github.com/MatthewInkawhich)

本文档提供关于存储和保存PyTorch模型的方法。随意阅读全文或者跳过一些内容，只读你关心的用例即可。

当谈到保存和加载模型时，有**三个核心函数**需要熟悉：

1. [torch.save](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save): **保存序列化的对象到磁盘**。这个函数使用Python的[pickle](https://docs.python.org/3/library/pickle.html)工具来序列化。模型，张量，以及字典等各种各样的对象都可以使用这个函数来存储。
2. [torch.load](https://pytorch.org/docs/stable/torch.html?highlight=torch load#torch.load): 使用[pickle](https://docs.python.org/3/library/pickle.html)的反序列化功能，可以将**对象文件加载到内存**。这个函数同时也能用于设备加载数据，(参考 [跨设备保存 & 加载模型](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices)).
3. [torch.nn.Module.load_state_dict](https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict): 使用反序列化的`state_dict`加载模型的参数字典，了解更多信息，阅读[什么是state_doct?](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).

**内容:**

- [什么是state_dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)
- [保存 & 加载模型用于推断](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference)
- [保存 & 加载通用的检查点](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training)
- [一个文件中保存多个模型](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-multiple-models-in-one-file)
- [从使用不同的模型的参数开始应用模型](https://pytorch.org/tutorials/beginner/saving_loading_models.html#warmstarting-model-using-parameters-from-a-different-model)
- [跨设备使用&加载模型](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices)

## 什么是`state_dict`?

在PyTorch中，`torch.nn.Module`类型的模型的可学习参数(如权重和偏置)保存在模型的参数中，可以通过`model.parameters()`访问。`state_dict`就是一个简单的Python原生字典对象，会将网络**的每层映射到参数张量**。注意，只有包含可学习参数的层(卷积层，全连接层等)以及注册的缓存(BN的运行时均值)在模型的`state_dict`中有保存。优化器对象(`torch.optim`)也有一个`state_dict`，包含了优化器的状态，以及用到的超参数。

因为`state_dict`对象是Python字典，可以轻松保存、更新、修改重新保存等，这样就为PyTorch模型和优化器增加了很多模块化的性质。

### 案例:

我们先从简单的模型来看看`state_dict`，这个模型是在[训练一个分类器](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)中用到的模型。

```python
# 定义模型
class TheModelClass(nn.Module):
    def __init__(self):
        super(TheModelClass, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 模型初始化
model = TheModelClass()

# 优化器初始化
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# 打印模型的state_dict
print("模型的state_dict:")
for param_tensor in model.state_dict():
    print(param_tensor, "\t", model.state_dict()[param_tensor].size())

# Print optimizer's state_dict
print("Optimizer's state_dict:")
for var_name in optimizer.state_dict():
    print(var_name, "\t", optimizer.state_dict()[var_name])
```

**输出:**

```bash
Model's state_dict:
conv1.weight     torch.Size([6, 3, 5, 5])
conv1.bias   torch.Size([6])
conv2.weight     torch.Size([16, 6, 5, 5])
conv2.bias   torch.Size([16])
fc1.weight   torch.Size([120, 400])
fc1.bias     torch.Size([120])
fc2.weight   torch.Size([84, 120])
fc2.bias     torch.Size([84])
fc3.weight   torch.Size([10, 84])
fc3.bias     torch.Size([10])

Optimizer's state_dict:
state    {}
param_groups     [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4675713712, 4675713784, 4675714000, 4675714072, 4675714216, 4675714288, 4675714432, 4675714504, 4675714648, 4675714720]}]
```

## 保存 & 加载模型用于推断

### 保存/加载 `state_dict` (推荐)

**保存:**

```python
torch.save(model.state_dict(), PATH)
```

**加载:**

```python
model = TheModelClass(*args, **kwargs) # 直接从源代码构建模型
model.load_state_dict(torch.load(PATH)) # 两重load
model.eval()
```

当保存模型用于推断时，只有保存训练模型的可学习参数是必须的。用`torch.save()`函数来保存模型的`state_dict`，能为我们提供重建模型时最大的灵活性。这也是为什么本方法是推荐的用于保存模型的方法。

通常PyTorch的约定是使用`.pt`或者`.pth`文件扩展来保存模型。

记住，在运行推断之前，你必须调用`model.eval()`来设置`dropout`和`BN`层到**评估模式**。如果不这么干会产生不一致的推断结果。

**注意**

`load_state_dict()`函数接收一个字典对象，不是保存模型的路径。也就是说在传入到`load_state_dict()`函数前，你必须反序列化保存的`state_dict`。比如，你不能这么加载：`model.load_state_dict(PATH)`。

### 保存/加载整个模型

**保存:**

```
torch.save(model, PATH)
```

**加载:**

```python
# 模型类必须在某处定义
model = torch.load(PATH)
model.eval()
```

这种保存/加载过程使用的是最直观的语法和最少的代码量。用这种方式保存一个模型会使用Python的`pickle`来保存整个模型模块。缺点是当保存模型时，序列化的数据和特定的类以及精确的目录结构绑定。原因自傲与`pickle`并不会保存模型类本身，而是保存一个路径到文件中，包含这个类，这会在加载时使用。因为这点，你的代码可能会以很多种方式崩溃，当将其用在其他项目中时。

通常约定PyTorch用`.pt`或者`.pth`文件扩展名来保存模型。

## 保存 & 加载一个通用的检查点用于推断或者继续训练

**保存:**

```python
torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            ...
            }, PATH)
```

**加载:**

```python
model = TheModelClass(*args, **kwargs)
optimizer = TheOptimizerClass(*args, **kwargs)

checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

model.eval()
# - or -
model.train()
```

当保存通用的检查点时，要么用于推断，要么用于继续训练，你必须保存模型的不止一个`state_dict`。保存模型的优化器的`state_dict`很重要，因为它包含了缓冲区数据以及随着模型训练时更新的参数。其他的想存储的数据有离开时的轮次，最后记录的训练损失，外部的`torch.nn.Embedding`层等。

为了保存多个组件，将它们组织为一个字典，然后使用`torch.save()`来序列化字典。通常PyTorch约定用`.tar`文件扩展名来保存这些检查点。

为了加载这些项目，首先需要初始化模型和优化器，然后从本地加载字典，使用`torch.load()`。从这里开始，你就能轻松访问保存的项目了，只需要查询这个字典即可。

记住，在运行推断之前，你必须调用`model.eval()`来设置`dropout`和`BN`层到评估模式。不这么做就会产生不一致的推断结果。如果你希望继续训练，调用`model.train()`来确保这些层处于训练模式。

PS：这样对比一下就很清楚了。训练时先执行下`model.train()`，推断时，先运行`model.eval()`。

## 保存多个模型到一个文件

**保存:**

```python
torch.save({
            'modelA_state_dict': modelA.state_dict(),
            'modelB_state_dict': modelB.state_dict(),
            'optimizerA_state_dict': optimizerA.state_dict(),
            'optimizerB_state_dict': optimizerB.state_dict(),
            ...
            }, PATH)
```

**加载：**

```python
modelA = TheModelAClass(*args, **kwargs)
modelB = TheModelBClass(*args, **kwargs)
optimizerA = TheOptimizerAClass(*args, **kwargs)
optimizerB = TheOptimizerBClass(*args, **kwargs)

checkpoint = torch.load(PATH)
modelA.load_state_dict(checkpoint['modelA_state_dict'])
modelB.load_state_dict(checkpoint['modelB_state_dict'])
optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])
optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])

modelA.eval()
modelB.eval()
# - or -
modelA.train()
modelB.train()
```

保存的模型由多个`torch.nn.Modules`组成时，如GAN，序列到序列模型，模型的集成等，保存模型用的是相同的方法。换句话说，保存每个模型的`state_dict`以及对应的优化器到字典中。前面提到的，你可以保存任何可能帮助你重建训练的项目到字典。

## 使用不同模型的参数开启一个新模型

**保存:**

```python
torch.save(modelA.state_dict(), PATH)
```

**加载:**

```python
modelB = TheModelBClass(*args, **kwargs)
modelB.load_state_dict(torch.load(PATH), strict=False)
```

在迁移学习或者训练一个复杂模型时，部分加载一个模型或者加载模型的一部分是很常见的场景。对于训练参数，即使是很少一部分可用，都会对训练过程有帮助，会让模型更快收敛，相比于从头开始训练。

无论你是从部分`state_dict`加载，这会丢失一些键值，或者加载多于模型本身的键值，可以在`load_state_dict()`函数中设定`strict`参数为`False`，来忽略不匹配的键。

如果你想将参数从一层加载到另外的层，但是有些键不匹配，在`state_dict`中简单修改参数键的名字即可。

## 跨设备保存 & 加载模型

### 保存在GPU，加载到CPU

**保存:**

```python
torch.save(model.state_dict(), PATH)
```

**加载:**

```python
device = torch.device('cpu')
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location=device))
```

当加载模型到CPU，而模型是在GPU上训练的，传递`torch.devive('cpu')`到`torch.load()`函数中的`map_location`字段即可。本例中，张量背后的存储是动态映射到CPU设备的。

### 保存到GPU，加载到GPU

**保存:**

```
torch.save(model.state_dict(), PATH)
```

**加载:**

```python
device = torch.device("cuda")
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.to(device)
# 确保调用了 input = input.to(device)，在任何时候需要将输入张量填充到模型时
```

加载在GPU上训练的模型到GPU，只需要将初始化的模型转化为CUDA优化的模型，使用`model.to(torch.device('cuda'))`。同时，确定使用`.to(torch.device('cuda'))`函数在所有的模型输入上，以准备好模型的数据。注意调用`my_tensor.to(device)`会返回在GPU上的一个拷贝。它不会重写`my_tensor`。因此，记住，手动重写`my_tensor = my_tensor.to(torch.device('cuda'))`。

### 保存到CPU，加载到GPU

**保存:**

```
torch.save(model.state_dict(), PATH)
```

**加载:**

```python
device = torch.device("cuda")
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location="cuda:0"))  # Choose whatever GPU device number you want
model.to(device)
# Make sure to call input = input.to(device) on any input tensors that you feed to the model
```

当加载在CPU上训练的模型到GPU时，设定`map_location`字段为`cuda:device_id`。这会将模型加载到给定的GPU设备。下一步，确保调用`model.to(torch.device('cuda'))`以转换模型参数张量为CUDA张量。最后，确保使用`.to(torch.device('cuda')))`函数来为输入到CUDA优化的模型准备好数据。注意调用`my_tensor.to(device)`会返回在GPU上的一个拷贝。它不会重写`my_tensor`。因此，记住，手动重写`my_tensor = my_tensor.to(torch.device('cuda'))`。

### 保存`torch.nn.DataParallel` 模型

**保存:**

```
torch.save(model.module.state_dict(), PATH)
```

**加载:**

```
# Load to whatever device you want
```

`torch.nn.DataParallel`是一个模型封装，它使得GPU能并行使用。为了保存`DataParallel`，需要保存`model.module.state_dict()`。这样，就能够有很大的灵活性来加载任何模型到任何设备。

END.